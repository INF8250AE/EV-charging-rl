{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DDQN + A2C + GA"
      ],
      "metadata": {
        "id": "vCyksZwsDuqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "cLq8Nms-0Wdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Double DQN"
      ],
      "metadata": {
        "id": "ljnohJTRSS8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class DDQNAgentTorch:\n",
        "    # Shared epsilon for all instances\n",
        "    common_epsilon = 1.0\n",
        "    common_epsilon_min = 0.0001\n",
        "    common_epsilon_decay = 0.995\n",
        "\n",
        "    def __init__(self,\n",
        "                 state_size,\n",
        "                 action_size,\n",
        "                 gamma=0.99,\n",
        "                 learning_rate=1e-3,\n",
        "                 memory_size=2500,\n",
        "                 device=None):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.gamma = gamma                    # discount factor\n",
        "        self.learning_rate = learning_rate\n",
        "        self.memory = deque(maxlen=memory_size)  # control memory size\n",
        "        self.loss = []\n",
        "\n",
        "        # Device: GPU if available, else CPU\n",
        "        if device is None:\n",
        "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        else:\n",
        "            self.device = torch.device(device)\n",
        "\n",
        "        # Online and target networks\n",
        "        self.model = self._build_model().to(self.device)\n",
        "        self.target_model = self._build_model().to(self.device)\n",
        "        self.update_target_model()  # copy initial weights\n",
        "\n",
        "        # Optimizer and loss\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "\n",
        "    def _build_model(self):\n",
        "        \"\"\"\n",
        "        state -> 2 layers -> Q(s, a) for all actions\n",
        "        \"\"\"\n",
        "        model = nn.Sequential(\n",
        "            nn.Linear(self.state_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, self.action_size)\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Store a transition in memory.\n",
        "        state, next_state should be 1D np arrays (state_size,)\n",
        "        action: int, reward: float, done: bool\n",
        "        \"\"\"\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def action(self, state):\n",
        "        \"\"\"\n",
        "        ε-greedy policy using the online model.\n",
        "        state is expected to be shape (state_size,) or (1, state_size).\n",
        "        Output: int action.\n",
        "        \"\"\"\n",
        "        # Explore\n",
        "        if np.random.rand() <= DDQNAgentTorch.common_epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "\n",
        "        # Exploit\n",
        "        if isinstance(state, np.ndarray):\n",
        "            state = torch.from_numpy(state).float()\n",
        "        state = state.to(self.device)\n",
        "\n",
        "        if state.dim() == 1:\n",
        "            state = state.unsqueeze(0)  # (1, state_size)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            q_values = self.model(state)  # (1, action_size)\n",
        "        action = torch.argmax(q_values[0]).item()\n",
        "        return action\n",
        "\n",
        "\n",
        "    def test_action(self, state):\n",
        "        \"\"\"\n",
        "        Greedy action (no exploration), for evaluation.\n",
        "        \"\"\"\n",
        "        if isinstance(state, np.ndarray):\n",
        "            state = torch.from_numpy(state).float()\n",
        "        state = state.to(self.device)\n",
        "\n",
        "        if state.dim() == 1:\n",
        "            state = state.unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            q_values = self.model(state)\n",
        "        action = torch.argmax(q_values[0]).item()\n",
        "        return action\n",
        "\n",
        "\n",
        "    def experience_replay(self, batch_size):\n",
        "        \"\"\"\n",
        "        Perform update using a minibatch from memory.\n",
        "        \"\"\"\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "\n",
        "        # Stack states and next_states for vectorized prediction\n",
        "        states = np.vstack([exp[0] for exp in minibatch])       # (B, state_size)\n",
        "        next_states = np.vstack([exp[3] for exp in minibatch])  # (B, state_size)\n",
        "\n",
        "        states = torch.from_numpy(states).float().to(self.device)       # (B, state_size)\n",
        "        next_states = torch.from_numpy(next_states).float().to(self.device)\n",
        "\n",
        "        # Q(s, ·) from online network\n",
        "        q_values = self.model(states)                   # (B, action_size)\n",
        "\n",
        "        # Q(s', ·) from online network (for argmax)\n",
        "        with torch.no_grad():\n",
        "            q_next_online = self.model(next_states)     # (B, action_size)\n",
        "            # Q(s', ·) from target network (for value)\n",
        "            q_next_target = self.target_model(next_states)  # (B, action_size)\n",
        "\n",
        "        # Build target Q-values\n",
        "        target_q = q_values.clone().detach()  # (B, action_size)\n",
        "\n",
        "        for idx, (_, action, reward, _, done) in enumerate(minibatch):\n",
        "\n",
        "            if done:\n",
        "                target = reward\n",
        "            else:\n",
        "                # Double DQN: choose action using online net, value from target net\n",
        "                best_next_action = torch.argmax(q_next_online[idx]).item()\n",
        "                target = reward + self.gamma * q_next_target[idx, best_next_action].item()\n",
        "\n",
        "            target_q[idx, action] = target\n",
        "\n",
        "        # Optimize the online network\n",
        "        self.optimizer.zero_grad()\n",
        "        loss = self.criterion(q_values, target_q)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.loss.append(loss.item())\n",
        "\n",
        "    @classmethod\n",
        "    def decay_epsilon(cls):\n",
        "        if cls.common_epsilon > cls.common_epsilon_min:\n",
        "            cls.common_epsilon *= cls.common_epsilon_decay\n",
        "\n",
        "    def update_target_model(self):\n",
        "        \"\"\"\n",
        "        Copy weights from online model to target model.\n",
        "        \"\"\"\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n"
      ],
      "metadata": {
        "id": "N5flSXlzSLGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DDQN Train loop over episodes"
      ],
      "metadata": {
        "id": "iELbc5poDSiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = EvChargingEnv(device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "state_size = env.observation_space[\"state\"].shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "DDQN_agent = DDQNAgentTorch(state_size, action_size)\n",
        "DDQN_return = []\n",
        "\n",
        "# As an example\n",
        "num_episodes = 100\n",
        "batch_size = 64\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    obs, info = env.reset()\n",
        "    state = obs[\"state\"].detach().cpu().numpy()  # (state_size,)\n",
        "\n",
        "    done = False\n",
        "    truncated = False\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not (done or truncated):\n",
        "        # choose action (int)\n",
        "        action = DDQN_agent.action(state)\n",
        "\n",
        "        # env expecrequires tensor action on env._device\n",
        "        action_tensor = torch.tensor(action, device=env._device)\n",
        "        next_obs, reward, done, truncated, info = env.step(action_tensor)\n",
        "\n",
        "        next_state = next_obs[\"state\"].detach().cpu().numpy()\n",
        "\n",
        "        reward = float(reward.item() if isinstance(reward, torch.Tensor) else reward)\n",
        "        episode_return += reward\n",
        "\n",
        "        DDQN_agent.remember(state, action, reward, next_state, done or truncated)\n",
        "\n",
        "        if len(DDQN_agent.memory) > batch_size:\n",
        "            DDQN_agent.experience_replay(batch_size=batch_size)\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "    DDQNAgentTorch.decay_epsilon()\n",
        "    DDQN_agent.update_target_model()   # At the end of each episode\n",
        "\n",
        "    DDQN_return.append(episode_return)\n",
        "    print(f\"Episode {episode}, return = {episode_return:.3f}\")\n"
      ],
      "metadata": {
        "id": "RZ4GssI5_XcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actor-Critic (A2C)"
      ],
      "metadata": {
        "id": "Qy-o6dE4mXAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "\n",
        "class ActorNet(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        logits = self.fc3(x)\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        return probs\n",
        "\n",
        "\n",
        "class CriticNet(nn.Module):\n",
        "    def __init__(self, state_size):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        value = self.fc3(x)   # (B, 1)\n",
        "        return value\n",
        "\n",
        "\n",
        "class ActorCriticAgent:\n",
        "    def __init__(self,\n",
        "                 state_size,\n",
        "                 action_size,\n",
        "                 gamma=0.99,\n",
        "                 actor_lr=1e-3,\n",
        "                 critic_lr=1e-3):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.gamma = gamma\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Build actor and critic neural networks\n",
        "        self.actor = ActorNet(state_size, action_size).to(self.device)\n",
        "        self.critic = CriticNet(state_size).to(self.device)\n",
        "\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
        "\n",
        "        self.actor_losses = []\n",
        "        self.critic_losses = []\n",
        "\n",
        "    # Action selection by actor NN\n",
        "    def select_action(self, state):\n",
        "        \"\"\"\n",
        "        state: np.array or tensor, shape (state_size,) or (1, state_size)\n",
        "        returns: action (int), log_prob (torch scalar)\n",
        "        \"\"\"\n",
        "        if isinstance(state, np.ndarray):\n",
        "            state = torch.from_numpy(state).float()\n",
        "        state = state.to(self.device)\n",
        "\n",
        "        if state.dim() == 1:\n",
        "            state = state.unsqueeze(0)  # (1, state_size)\n",
        "\n",
        "        probs = self.actor(state).squeeze(0)   # (action_size,)\n",
        "\n",
        "        dist = Categorical(probs)\n",
        "        action = dist.sample()\n",
        "        log_prob = dist.log_prob(action)  # scalar\n",
        "\n",
        "        return action.item(), log_prob\n",
        "\n",
        "    def greedy_action(self, state):\n",
        "        \"\"\"Deterministic (argmax) action for evaluation.\"\"\"\n",
        "        if isinstance(state, np.ndarray):\n",
        "            state = torch.from_numpy(state).float()\n",
        "        state = state.to(self.device)\n",
        "\n",
        "        if state.dim() == 1:\n",
        "            state = state.unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            probs = self.actor(state).squeeze(0)\n",
        "        action = torch.argmax(probs).item()\n",
        "        return action\n",
        "\n",
        "    # A2C update (update actor and critic NNs each time step)\n",
        "    def train_step(self, state, action_log_prob, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        A2C update (one-step TD):\n",
        "          td_target = r + gamma * V(next_state) * (1 - done)\n",
        "          td_error  = td_target - V(state)\n",
        "\n",
        "        actor_loss  = -log π(a|s) * td_error.detach()\n",
        "        critic_loss = td_error**2\n",
        "        \"\"\"\n",
        "        # Convert to tensors\n",
        "        if isinstance(state, np.ndarray):\n",
        "            state = torch.from_numpy(state).float()\n",
        "        if isinstance(next_state, np.ndarray):\n",
        "            next_state = torch.from_numpy(next_state).float()\n",
        "\n",
        "        state = state.to(self.device)\n",
        "        next_state = next_state.to(self.device)\n",
        "\n",
        "        if state.dim() == 1:\n",
        "            state = state.unsqueeze(0)\n",
        "        if next_state.dim() == 1:\n",
        "            next_state = next_state.unsqueeze(0)\n",
        "\n",
        "        reward = torch.tensor([reward], dtype=torch.float32, device=self.device)\n",
        "        done = torch.tensor([float(done)], dtype=torch.float32, device=self.device)\n",
        "\n",
        "        # Critic values\n",
        "        value = self.critic(state).squeeze(1)        # (1,)\n",
        "        next_value = self.critic(next_state).squeeze(1)  # (1,)\n",
        "\n",
        "        td_target = reward + self.gamma * next_value * (1.0 - done)\n",
        "        td_error = td_target - value                 # (1,)\n",
        "\n",
        "        # Critic loss (MSE)\n",
        "        critic_loss = (td_error.pow(2)).mean()\n",
        "\n",
        "        #update critic network's parameters\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        # Actor loss (advantage = td_error.detach())\n",
        "        advantage = td_error.detach()\n",
        "        actor_loss = -(action_log_prob * advantage).mean()\n",
        "\n",
        "        #update actor network's parameters\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        self.actor_losses.append(actor_loss.item())\n",
        "        self.critic_losses.append(critic_loss.item())"
      ],
      "metadata": {
        "id": "Es3ZHGEwmULN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### A2C Train loop over episodes"
      ],
      "metadata": {
        "id": "LQL8mgCoDcP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = EvChargingEnv(device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "state_size = env.observation_space[\"state\"].shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "A2C_agent = ActorCriticAgent(state_size, action_size)\n",
        "A2C_return = []\n",
        "\n",
        "num_episodes = 100\n",
        "\n",
        "for ep in range(num_episodes):\n",
        "    obs, info = env.reset()\n",
        "    state = obs[\"state\"].detach().cpu().numpy()\n",
        "\n",
        "    done = False\n",
        "    truncated = False\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not (done or truncated):\n",
        "        action_idx, log_prob = A2C_agent.select_action(state)\n",
        "\n",
        "        # env expects a torch.Tensor action on env._device\n",
        "        action_tensor = torch.tensor(action_idx, device=env._device)\n",
        "        next_obs, reward, done, truncated, info = env.step(action_tensor)\n",
        "\n",
        "        next_state = next_obs[\"state\"].detach().cpu().numpy()\n",
        "\n",
        "        episode_return += float(reward.item() if isinstance(reward, torch.Tensor) else reward)\n",
        "        A2C_return.append(episode_return)\n",
        "\n",
        "        done_flag = done or truncated\n",
        "\n",
        "        A2C_agent.train_step(state, log_prob, reward, next_state, done_flag)\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "    print(f\"Episode {ep}, return = {episode_return:.3f}\")\n"
      ],
      "metadata": {
        "id": "sao_cEFXBa-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Genetic Algorithm Baseline (Heuristic model)\n"
      ],
      "metadata": {
        "id": "RiPWLuouX6Rd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def heuristic_action_from_genome(genome, env, obs):\n",
        "    \"\"\"\n",
        "    genome: np.array of shape (6,) -> weights [w0,w1,w2,w3,w4,w5]\n",
        "    env: EvChargingEnv instance\n",
        "    obs: observation dict from env ({\"state\": torch.Tensor})\n",
        "\n",
        "    Output: int action = selected station index\n",
        "    \"\"\"\n",
        "    state_tensor = obs[\"state\"]\n",
        "\n",
        "    # Convert to numpy on CPU\n",
        "    if isinstance(state_tensor, torch.Tensor):\n",
        "        state_np = state_tensor.detach().cpu().numpy()\n",
        "    else:\n",
        "        state_np = np.array(state_tensor, dtype=np.float32)\n",
        "\n",
        "\n",
        "    # observation:[ steps_until_next_arrival, car_to_route_state(5), station_0_state, station_1_state, ... ]\n",
        "    steps_until_next = state_np[0]\n",
        "    car_vec = state_np[1:1 + NB_STATE_PER_CAR]  # length 5\n",
        "\n",
        "    travel_time_norm = car_vec[0]\n",
        "    soc = car_vec[1]\n",
        "    desired_soc = car_vec[2]\n",
        "    capacity_norm = car_vec[3]\n",
        "    urgency = car_vec[4]\n",
        "\n",
        "    # If there is no car to route (filler = -1), action doesn't matter → choose 0\n",
        "    if travel_time_norm == FILLER_VALUE:\n",
        "        return 0\n",
        "\n",
        "    scores = []\n",
        "\n",
        "    # genome = [w0, w1, w2, w3, w4, w5]\n",
        "    w0, w1, w2, w3, w4, w5 = genome\n",
        "\n",
        "    # Normalize features in get_state())\n",
        "    for station in env.stations:\n",
        "        station_state_t = station.get_state()                # torch.Tensor\n",
        "        station_state = station_state_t.detach().cpu().numpy()\n",
        "\n",
        "        # First 8 entries = station_only_state:[charge_speed_norm, charge_speed_sharpness_norm, nb_free_chargers_norm, nb_cars_traveling_norm,\n",
        "        #  nb_cars_charging_norm, nb_cars_waiting_norm, is_max_nb_cars_traveling_reached, is_max_nb_cars_waiting_reached]\n",
        "        charge_speed_norm      = station_state[0]\n",
        "        nb_free_chargers_norm  = station_state[2]\n",
        "        nb_cars_waiting_norm   = station_state[5]\n",
        "\n",
        "        # Define features for scoring\n",
        "        x1 = charge_speed_norm\n",
        "        x2 = nb_free_chargers_norm\n",
        "        x3 = nb_cars_waiting_norm\n",
        "        x4 = urgency\n",
        "        x5 = 1.0 - soc\n",
        "\n",
        "        # Linear scoring heuristic\n",
        "        score = w0 + w1 * x1 + w2 * x2 - w3 * x3 + w4 * x4 + w5 * x5\n",
        "        scores.append(score)\n",
        "\n",
        "    best_station = int(np.argmax(scores))\n",
        "    return best_station\n",
        "\n",
        "\n",
        "def evaluate_genome(genome, env, n_eval_episodes=3, max_steps_per_episode=None):\n",
        "    total_return = 0.0\n",
        "\n",
        "    for ep in range(n_eval_episodes):\n",
        "        obs, info = env.reset()\n",
        "\n",
        "        episode_return = 0.0\n",
        "        done = False\n",
        "        truncated = False\n",
        "\n",
        "        while not (done or truncated):\n",
        "\n",
        "            # Select action from heuristic\n",
        "            action_idx = heuristic_action_from_genome(genome, env, obs)\n",
        "            action_tensor = torch.tensor(action_idx, device=env._device) # Env needs a torch.Tensor action\n",
        "\n",
        "            obs, reward, done, truncated, info = env.step(action_tensor)\n",
        "\n",
        "            reward = float(reward.item()) if isinstance(reward, torch.Tensor) else reward  # reward: torch scalar -> float\n",
        "            episode_return += reward\n",
        "\n",
        "        total_return += episode_return\n",
        "\n",
        "    mean_return = total_return / n_eval_episodes\n",
        "\n",
        "    return mean_return\n",
        "\n"
      ],
      "metadata": {
        "id": "tcyL3cgBX9p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GeneticAlgorithmEV:\n",
        "    def __init__(\n",
        "        self,\n",
        "        env,\n",
        "        genome_dim=6,\n",
        "        population_size=20,\n",
        "        n_generations=10,\n",
        "        elite_frac=0.2,    # The top 20% of genomes (by fitness) survive to the next generation\n",
        "        mutation_std=0.1,  # Standard deviation of Gaussian noise added to genome values during mutation.\n",
        "        mutation_prob=0.3, # Probability that a newly created child (via crossover) gets mutated -> 30% get Gaussian noise, others unchanged\n",
        "        n_eval_episodes=3,\n",
        "        seed=42,\n",
        "    ):\n",
        "        self.genome_dim = genome_dim\n",
        "        self.population_size = population_size\n",
        "        self.n_generations = n_generations\n",
        "        self.elite_frac = elite_frac\n",
        "        self.mutation_std = mutation_std\n",
        "        self.mutation_prob = mutation_prob\n",
        "        self.n_eval_episodes = n_eval_episodes\n",
        "\n",
        "        # Fixed random seed for reproducibility\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        self.best_genome = None\n",
        "        self.best_fitness = -np.inf\n",
        "        self.best_fitness_history = []\n",
        "\n",
        "    def _init_population(self):\n",
        "        # Uniform init in [-1, 1]\n",
        "        return np.random.uniform(-1.0, 1.0, size=(self.population_size, self.genome_dim))\n",
        "\n",
        "    def _evaluate_population(self, population):\n",
        "        fitnesses = []\n",
        "        for genome in population:\n",
        "            fit = evaluate_genome(genome, env, n_eval_episodes=self.n_eval_episodes)\n",
        "            fitnesses.append(fit)\n",
        "        return np.array(fitnesses)\n",
        "\n",
        "    def run(self):\n",
        "        population = self._init_population()\n",
        "        n_elite = max(1, int(self.elite_frac * self.population_size))\n",
        "\n",
        "        for gen in range(self.n_generations):\n",
        "            fitnesses = self._evaluate_population(population)\n",
        "\n",
        "            # Track best\n",
        "            best_idx = np.argmax(fitnesses)\n",
        "            best_fit = fitnesses[best_idx]\n",
        "            best_gen = population[best_idx]\n",
        "\n",
        "            if best_fit > self.best_fitness:\n",
        "                self.best_fitness = best_fit\n",
        "                self.best_genome = best_gen.copy()\n",
        "\n",
        "            self.best_fitness_history.append(self.best_fitness)\n",
        "\n",
        "            print(f\"[GA] Gen {gen} | gen_best={best_fit:.3f} | overall_best={self.best_fitness:.3f}\")\n",
        "\n",
        "            # Select elites\n",
        "            elite_indices = np.argsort(fitnesses)[-n_elite:]\n",
        "            elites = population[elite_indices]\n",
        "\n",
        "            # Build new population\n",
        "            new_pop = [elites[0]]   # keep the best individual always\n",
        "\n",
        "            while len(new_pop) < self.population_size:\n",
        "                # Select two parents randomly from elite set\n",
        "                p1, p2 = elites[np.random.randint(0, n_elite)], elites[np.random.randint(0, n_elite)]\n",
        "\n",
        "                # Uniform crossover\n",
        "                mask = np.random.rand(self.genome_dim) < 0.5\n",
        "                child = np.where(mask, p1, p2)\n",
        "\n",
        "                # Mutation\n",
        "                if np.random.rand() < self.mutation_prob:\n",
        "                    child = child + np.random.normal(0, self.mutation_std, size=self.genome_dim)\n",
        "\n",
        "                new_pop.append(child)\n",
        "\n",
        "            population = np.array(new_pop)\n",
        "\n",
        "        return self.best_genome, self.best_fitness, self.best_fitness_history\n",
        "\n",
        "\n",
        "    def _init_population(self):\n",
        "        # Initialize weights in [-1, 1]\n",
        "        pop = self.rng.uniform(-1.0, 1.0, size=(self.population_size, self.genome_dim))\n",
        "        return pop\n",
        "\n",
        "    def _evaluate_population(self, population):\n",
        "        fitnesses = []\n",
        "        for i in range(len(population)):\n",
        "            genome = population[i]\n",
        "            fit = evaluate_genome(\n",
        "                genome,\n",
        "                n_eval_episodes=self.n_eval_episodes,\n",
        "                env_kwargs=self.env_kwargs,\n",
        "            )\n",
        "            fitnesses.append(fit)\n",
        "        return np.array(fitnesses)\n",
        "\n",
        "    def run(self):\n",
        "        population = self._init_population()\n",
        "        n_elite = max(1, int(self.elite_frac * self.population_size))\n",
        "\n",
        "        for gen in range(self.n_generations):\n",
        "            # 1) Evaluate\n",
        "            fitnesses = self._evaluate_population(population)\n",
        "\n",
        "            # 2) Track best\n",
        "            gen_best_idx = int(np.argmax(fitnesses))\n",
        "            gen_best_fit = fitnesses[gen_best_idx]\n",
        "            gen_best_genome = population[gen_best_idx].copy()\n",
        "\n",
        "            if gen_best_fit > self.best_fitness:\n",
        "                self.best_fitness = gen_best_fit\n",
        "                self.best_genome = gen_best_genome.copy()\n",
        "\n",
        "            self.best_fitness_history.append(self.best_fitness)\n",
        "            print(\n",
        "                f\"[GA] Generation {gen}/{self.n_generations-1} \"\n",
        "                f\"best_gen_fit = {gen_best_fit:.3f}, \"\n",
        "                f\"overall_best = {self.best_fitness:.3f}\"\n",
        "            )\n",
        "\n",
        "            # 3) Selection: take elites\n",
        "            elite_indices = np.argsort(fitnesses)[-n_elite:]\n",
        "            elites = population[elite_indices]\n",
        "\n",
        "            # 4) Create new population\n",
        "            new_pop = [elites[0]]  # keep the single best (elitism)\n",
        "\n",
        "            while len(new_pop) < self.population_size:\n",
        "                # parents: random elites\n",
        "                p1, p2 = elites[self.rng.randint(0, n_elite, size=2)]\n",
        "\n",
        "                # uniform crossover\n",
        "                mask = self.rng.rand(self.genome_dim) < 0.5\n",
        "                child = np.where(mask, p1, p2)\n",
        "\n",
        "                # mutation\n",
        "                if self.rng.rand() < self.mutation_prob:\n",
        "                    noise = self.rng.normal(0.0, self.mutation_std, size=self.genome_dim)\n",
        "                    child = child + noise\n",
        "\n",
        "                new_pop.append(child)\n",
        "\n",
        "            population = np.array(new_pop)\n",
        "\n",
        "        return self.best_genome, self.best_fitness, self.best_fitness_history\n",
        "\n"
      ],
      "metadata": {
        "id": "_i-VF-J_c8DW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GA Evaluation"
      ],
      "metadata": {
        "id": "vKC9x8nJo4X4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_best_genome(best_genome, env, n_eval_episodes=20):\n",
        "    returns = []\n",
        "\n",
        "    for ep in range(n_eval_episodes):\n",
        "        obs, info = env.reset()\n",
        "\n",
        "        episode_return = 0.0\n",
        "        done = False\n",
        "        truncated = False\n",
        "\n",
        "        while not (done or truncated):\n",
        "            action_idx = heuristic_action_from_genome(best_genome, env, obs)\n",
        "            action_tensor = torch.tensor(action_idx, device=env._device)\n",
        "\n",
        "            obs, reward, done, truncated, info = env.step(action_tensor)\n",
        "\n",
        "            if isinstance(reward, torch.Tensor):\n",
        "                r = float(reward.item())\n",
        "            else:\n",
        "                r = float(reward)\n",
        "            episode_return += r\n",
        "\n",
        "            # Optional safety stop if you want\n",
        "            if env.step_count >= env.max_steps:\n",
        "                break\n",
        "\n",
        "        returns.append(episode_return)\n",
        "\n",
        "    returns = np.array(returns)\n",
        "    print(f\"Final GA heuristic evaluation over {n_eval_episodes} episodes:\")\n",
        "    print(f\"  Mean return = {returns.mean():.3f}\")\n",
        "    print(f\"  Std return  = {returns.std():.3f}\")\n",
        "    return returns\n"
      ],
      "metadata": {
        "id": "UqxwsvsziDuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GA Train loop over episodes + Evaluation phase"
      ],
      "metadata": {
        "id": "pfgYbuSYDli-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Create ONE environment instance\n",
        "    env = EvChargingEnv(\n",
        "        device=\"cpu\",\n",
        "        seed=123,\n",
        "        nb_stations=5,\n",
        "        max_steps=1000,\n",
        "        # add other parameters if you changed defaults\n",
        "    )\n",
        "\n",
        "    # Run GA to optimize heuristic weights\n",
        "    ga = GeneticAlgorithmEV(\n",
        "        env=env,\n",
        "        genome_dim=6,\n",
        "        population_size=20,\n",
        "        n_generations=10,\n",
        "        elite_frac=0.3,\n",
        "        mutation_std=0.1,\n",
        "        mutation_prob=0.3,\n",
        "        n_eval_episodes=2,   # small for speed; you can increase later\n",
        "        seed=42,\n",
        "    )\n",
        "\n",
        "    best_genome, best_fit, fitness_history = ga.run()\n",
        "\n",
        "    print(\"Best genome (weights):\", best_genome)\n",
        "    print(\"Best fitness (mean return during GA eval):\", best_fit)\n",
        "\n",
        "    # Plot GA best fitness over generations\n",
        "    plt.figure()\n",
        "    plt.plot(fitness_history, marker='o')\n",
        "    plt.xlabel(\"Generation\")\n",
        "    plt.ylabel(\"Best mean return\")\n",
        "    plt.title(\"GA best fitness per generation\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Final evaluation of best heuristic policy\n",
        "    final_returns = evaluate_best_genome(\n",
        "        best_genome,\n",
        "        env,\n",
        "        n_eval_episodes=20,\n",
        "    )\n"
      ],
      "metadata": {
        "id": "lEunlte8pKHQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}